---
title: 卒論の原罪
date: 2022-03-25
---

# 卒論の原罪

卒論も受理され卒業も確定したので振り返ってみる。
特に僕と同様の研究をしようと思っている人の参考になれば幸いである。


## 研究背景

ECサイトで商品の購入検討する時、実物を見れないから
実際に買った人のレビューって参考にならない？(僕は参考にしてる)
その時、本当に欲しい情報って肯定的なものだけでなく否定的なもの、
もっと言うとなんでそう評価したのか、ってことが知りたい。
でも楽天なんかで見てみると、目に付きやすいとこにあるレビューは
高評価なレビューばかりでバイアスがあるように感じる。
アマゾンなんかだと「このレビューが参考になった」ってボタンで投票ができて
票数が多いレビューは購入するときの参考になったりするけど、
この機能ってプラットフォーム運営側としては事業としてクリティカルではないし
データを十分集められる保証も無いために機能提供しているとこは実際少ない。
そこで、参考になるレビューをレコメンドするために、
レビュー文を自動で判定する機械があると嬉しいよねって発想に至った。


## 目的

ユーザーの投稿するレビューを解析して、
他のユーザーが商品購入する際の参考資料として適当な
レビューであるか判定するモデルを作る。

注意すべきは、このモデルが判定しているのは
「そのレビューが肯定的な内容を書いているか否か」ではなく
「なぜそのレビュー評価(例えば5段階評価で1を付ける、逆に5を付ける)
に至ったのか具に説明している」である。
(この時点で致命的な事があるのだが後に説明する。)

また卒論発表のときに突っ込まれたけど、ターゲティング広告みたいに
個人個人に合わせてレコメンドするものでもない。
世間一般的に参考になるかどうかの判定が目的。


## やったこと

### huggingfaceのBERTを使ってみる

まずは普通にやった場合どのぐらいの精度が出るのか、
ライブラリの使い方に慣れるという点も含め、
huggingfaceで配布されている事前学習済みBERTモデルで
レビュー文全体の特徴量を抽出し二値分類を行った。(卒論で書いた提案手法1に該当)
この時が確か75%前後？で予測できたと卒論に書いた気がする。
(僕の卒論読める人はそっちを確認して欲しい)
この精度、実は試行の度にバラついてかなり怪しかった。
あるときは65%とか、またあるときは90%弱とか。
手始めにBERT触った感覚としては「BERTすげぇ!!!、けどこのばらつきなんだ??」ってずっと言ってた。

### 提案手法を考える

一般的にBERTは一回の入力に2文までしか考慮されていない。
(学習のやり方にもよるのだが)
それに対し、レビューというのは大抵2文で収まるようなものではなく、
レビュー全体から重要な2文だけ選択してレビュー全体の代表とするというのも
無理やりな気がする。
ここまでのことをまとめると、BERTを使って、レビュー全体から特徴量を取る手法
というのが要件になる。
ここで指導教授から「レビューを一文ずつBERTで特徴抽出し、
得られたものを結合してTransformerに入力してみたら？」という助言を頂いた。
要するに以下の図の感じ。(卒論で書いた提案手法3に該当)

![proposed_method](https://user-images.githubusercontent.com/58410530/151568274-81e7783c-7b94-4a25-b8d4-42b98771553c.png)

1. レビュー全体を構文解析通して一文ずつに分割
2. 一文ずつトークン化、それをBERTに通して文の特徴量を獲得
3. 得られた特徴量のうち、CLSトークンに対応する出力のみ選択
4. 各文の3の結果を合体&特異トークンなどを付与して所定のサイズに整形
5. 4の出力を4層のTransformerに入れてレビュー全体の特徴量を獲得
6. 続けて全結合でゴニョゴニョして、票数が一定以上あるないで二値分類

というのが提案手法の流れ。

ちなみに提案手法2は提案手法3のTranformerをCNNに置き換えた対照実験になっている。


### 実装、計測

ここからはこんな感じの同じ日課を3週間ぐらい過ごした。

1. 午前中に実装する
2. 計測開始、待ち時間に卒論書き進める
3. 計測結果のまとめ
4. 次の計画、修正箇所を考え翌日のタスクに積む

一人暮らしで人との会話もないし、コロナで籠もりっきりなのも相まってまじで辛かった。


## 計測結果

提案手法1 > 提案手法3 > 提案手法2 の順で精度が良いという結果だった。
正直、3 > 2 > 1とかの順になってくれれば万々歳だったけど、
世の中そう上手くは行かないんですね。


## 実際のところ

### 筋書き

卒論では提案手法を3つ出したことになっている。
でも実は提案手法は1つの予定で、ここまで読めば分かるとおり提案手法3がそれになる。

当初の筋書きでは、
提案手法1の内容で、利用する事前学習済みBERTモデルの性能の確認検証、
提案手法2の内容で、提案手法の対照実験、その結果、
3 > 2 > 1の順で精度良く予測できて、「BERTを使って3文以上から特徴抽出できる!」、
「CNNよりTransformer良いね!」って結論に着地したかった。

けど実際の計測結果は芳しくなく何度も試行錯誤したけどうまく行かなかった。
時間が足りず大きな変更はできなかった。
まだまだ試したいことはあったけどここでタイムアウト、
聞こえ良くなるよういい感じに書き換えて提出した。


### 提案手法3の予測要因

なぜ提案手法3がうまく行かなかったか。
というより何を指標に予測しているか。
おそらく、文章長で良いレビューか判断していたと思う。
参考にならないレビューは「よかった」とか「買わないほうがいい」とか短文が多いし、
逆に参考になるレビューは、なぜその評価になったかの説明で文が長くなる。
このことを検証するため、一文ごとにBERTで取れるCLSトークンと対になる出力を全て
任意のトークン(例えば「1, 0, 1, 0, ...」)に置き換えて同様に計測してみた。
つまりここでは文章数だけで予測できるのか検証している。
この結果、置き換える前と全く同じ精度になった。
文ごとの特徴量を使っても結局は文章数しか見てない、という可能性が大きいことがわかった。

ちなみに、卒論にはこのこと書いたけど自分の研究を否定することは
書かないほうがいいということで提出前に消した。


## じゃあどうすればよかったの？

![wakannaippi](https://user-images.githubusercontent.com/58410530/159123475-2a7eeba4-810a-4282-9964-1f9bade4985a.jpg)
(この画像を使いたかっただけのタイトル)

と言いつつちょっと真面目に考えてみる。

卒論最終章にはいろいろ書いたけど結局のところ、下の2つが無難な気がする。

- 学習データは全て長文のもののみ利用する
  - 長文かつ参考にならないレビューって少なそうだけど
- BERTをファインチューニングする

でもまあ正直ファインチューニングってどこまで効果あるか未知数だし
前者を試してダメそうだったら研究テーマ変えた方が良さそう。
そんなとこ。

最後まで読んでくれてありがとう。


### 追記

卒論に使ったコードは下のリンク先に置いておきます。
わからないことあったら気軽に[連絡ください](mailto:wetsand.wfs@gmail.com)。

[ReviewVital](https://github.com/kakubin/review_vital)
